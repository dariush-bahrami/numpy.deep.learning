{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEVWWGAytvXA"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_adi4DUtvXD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBeRwmdutvXO"
   },
   "source": [
    "# Structure Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrEsUnixtvXQ"
   },
   "source": [
    "## Activation Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icI3PddDtvXR"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # negative_overflow = np.where(z<=-709, 0, 0)\n",
    "    okay_range = np.where((-709<z)&(z<709), 1/(1 + np.exp(-z)), 0)\n",
    "    positive_overflow = np.where(z>=709, 1, 0)\n",
    "    return okay_range + positive_overflow\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "def relu(z, leak_grad=0):\n",
    "    return np.maximum(leak_grad*z, z)\n",
    "\n",
    "\n",
    "def relu_derivative(z, leak_grad=0):\n",
    "    return np.where(z <= 0, leak_grad, 1)\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "\n",
    "def softmax(Z, epsilon=1e-8):\n",
    "    Z = Z - np.max(Z)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / (np.sum(exp_Z, axis=0)+epsilon)\n",
    "\n",
    "# def softmax_derivative(Z):\n",
    "#     return 1\n",
    "\n",
    "def identity(Z):\n",
    "    return Z\n",
    "\n",
    "def identity_derivative(Z):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToG5-ziotvXY"
   },
   "outputs": [],
   "source": [
    "activation_functions_dict = {'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "                             'relu': (relu, relu_derivative),\n",
    "                             'tanh': (tanh, tanh_derivative),\n",
    "                             'softmax': (softmax, None),\n",
    "                             'identity': (identity, identity_derivative)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A53MQON8tvXh"
   },
   "source": [
    "## Layer Namespace Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aVttHGxtvXi"
   },
   "outputs": [],
   "source": [
    "class LayerNamespace(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CotRqlYStvXo"
   },
   "source": [
    "## Structure Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVGGGu36tvXp"
   },
   "outputs": [],
   "source": [
    "class ModelStructure(object):\n",
    "    def __init__(self):\n",
    "        self.__layers = []\n",
    "\n",
    "    def add_input_layer(self, input_features: int):\n",
    "        assert len(self.__layers) == 0, 'Add input layer before other layers'\n",
    "        assert type(input_features) == int, 'Enter integer number'\n",
    "        layer_0 = LayerNamespace()\n",
    "        layer_0.units_number = input_features\n",
    "        self.__layers.append(layer_0)\n",
    "\n",
    "    def add_layer(self, units_number: int, activation_function_name='identity',\n",
    "                  keep_prob=1):\n",
    "        assert type(units_number) == int, 'Enter integer number'\n",
    "        assert len(self.__layers) > 0, 'First add input layer'\n",
    "\n",
    "        try:\n",
    "            layer = LayerNamespace()\n",
    "            valid_key = activation_function_name.lower()\n",
    "            layer.activation_function = activation_functions_dict[valid_key][0]\n",
    "            layer.activation_function_derivative = activation_functions_dict[valid_key][1]\n",
    "            layer.units_number = units_number\n",
    "            layer.keep_prob = keep_prob\n",
    "            self.__layers.append(layer)\n",
    "        except KeyError as error:\n",
    "            message = 'Valid activation functions: '\n",
    "            message += ', '.join(activation_functions_dict.keys())\n",
    "            raise KeyError(f'{error} ({message})')\n",
    "\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        assert len(self.__layers) > 1, 'At least nsert one layer'\n",
    "        return self.__layers\n",
    "\n",
    "    def summary(self):\n",
    "        print(f'Input Features: {self.layers[0].units_number}')\n",
    "        total_parameters = 0\n",
    "        pre_units = self.layers[0].units_number\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            weights = layer.units_number * pre_units\n",
    "            biases = layer.units_number\n",
    "            message_parts = [f'Layer #{i+1}:',\n",
    "                             f'{layer.units_number}',\n",
    "                             f'{layer.activation_function.__name__}',\n",
    "                             f'units with {weights} weights and',\n",
    "                             f'{biases} biases']\n",
    "            print(' '.join(message_parts))\n",
    "            pre_units = layer.units_number\n",
    "            total_parameters += (weights+biases)\n",
    "        print(f'Total Parameters: {total_parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-RlP8zmtvX-"
   },
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOIA9vhitvX_"
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMwEotMQDRig"
   },
   "source": [
    "### Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQdsZ1fytvYA"
   },
   "outputs": [],
   "source": [
    "def initialize(layers: list):\n",
    "    result = []\n",
    "    previous_layer = layers[0]\n",
    "    for layer in layers[1:]:\n",
    "        shape = (layer.units_number,\n",
    "                 previous_layer.units_number)\n",
    "        # On weight initialization in deep neural networks\n",
    "        scale_dict = {'relu': np.sqrt(2/shape[1]),\n",
    "                      'tanh': np.sqrt(1/shape[1]),\n",
    "                      'sigmoid': np.sqrt((3.6**2)/shape[1]),\n",
    "                      'softmax': np.sqrt(2/shape[1]),\n",
    "                      'identity': np.sqrt(2/shape[1])}\n",
    "        func_name = layer.activation_function.__name__\n",
    "        layer.W = np.random.randn(\n",
    "            shape[0], shape[1]) * scale_dict[func_name]\n",
    "\n",
    "        layer.b = np.zeros((shape[0], 1))\n",
    "        result.append(layer)\n",
    "        previous_layer = layer\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW3IOak9DRiy"
   },
   "source": [
    "### Mini Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vku6MRgGDRi0"
   },
   "outputs": [],
   "source": [
    "def mini_batch_generator(x: np.ndarray, y: np.ndarray, batch_size: int):\n",
    "    assert x.shape[1] == y.shape[1]\n",
    "    m = x.shape[1]\n",
    "    random_indice = np.random.permutation(m)\n",
    "    shuffled_x = x[:, random_indice]\n",
    "    shuffled_y = y[:, random_indice]\n",
    "\n",
    "    div = divmod(m, batch_size)\n",
    "    for i in range(div[0]):\n",
    "        x_mini_batch = shuffled_x[:, i*batch_size:(i+1)*batch_size]\n",
    "        x_mini_batch = x_mini_batch.reshape(x.shape[0], batch_size)\n",
    "\n",
    "        y_mini_batch = shuffled_y[:, i*batch_size:(i+1)*batch_size]\n",
    "        y_mini_batch = y_mini_batch.reshape(y.shape[0], batch_size)\n",
    "\n",
    "        yield x_mini_batch, y_mini_batch\n",
    "\n",
    "    if div[1]:\n",
    "        x_mini_batch = shuffled_x[:, div[0]*batch_size:]\n",
    "        x_mini_batch = x_mini_batch.reshape(x.shape[0], div[1])\n",
    "\n",
    "        y_mini_batch = shuffled_y[:, div[0]*batch_size:]\n",
    "        y_mini_batch = y_mini_batch.reshape(y.shape[0], div[1])\n",
    "\n",
    "        yield x_mini_batch, y_mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onpcFF8tDRi7"
   },
   "source": [
    "### Metric Calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiQc8dw6DRi8"
   },
   "outputs": [],
   "source": [
    " def get_predict_function(apply_softmax=True):   \n",
    " \n",
    "    def predict(model_output):\n",
    "        prediction = np.where(model_output > 0.5, 1, 0)\n",
    "        return prediction\n",
    "\n",
    "    def softmax_predict(Z):\n",
    "        A = softmax(Z)\n",
    "        return predict(A)\n",
    "\n",
    "    if apply_softmax:\n",
    "        return softmax_predict\n",
    "    else:\n",
    "        return predict\n",
    "\n",
    "def get_accuracy_function(binary=False):\n",
    "    def binary_accuracy(prediction, expected):\n",
    "        assert prediction.shape == expected.shape\n",
    "    #     assert prediction.shape[0] == 1\n",
    "        m = expected.shape[1]\n",
    "        accuracy = np.sum(prediction == expected)/m\n",
    "        return accuracy\n",
    "\n",
    "    def categorical_accuracy(prediction, expected):\n",
    "        assert prediction.shape == expected.shape\n",
    "        assert prediction.shape[0] != 1\n",
    "        m = expected.shape[1]\n",
    "        accuracy = np.sum(np.argmax(prediction, axis=0) == np.argmax(expected, axis=0))/m\n",
    "        return accuracy\n",
    "\n",
    "    if binary:\n",
    "        return binary_accuracy\n",
    "    else: \n",
    "        return categorical_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOjg-UbgDRjG"
   },
   "source": [
    "### Metric Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahvT82NiDRjJ"
   },
   "outputs": [],
   "source": [
    "def print_metrics(interval=100):\n",
    "    def result_function(metrics):\n",
    "        epoch = metrics['total_trained_epochs']\n",
    "        if (epoch == 1) or (epoch % interval == 0):\n",
    "            cost = metrics['costs'][-1]\n",
    "            accuracy = metrics['accuracies'][-1]*100\n",
    "            message_parts = [f'Epoch #{epoch:0>4}',\n",
    "                             f'Cost: {cost:.4f}',\n",
    "                             f'Accuracy: {accuracy:.2f}%']\n",
    "\n",
    "            if metrics['validation_costs']:\n",
    "                validation_cost = metrics['validation_costs'][-1]\n",
    "                validation_accuracy = metrics['validation_accuracies'][-1]*100\n",
    "                message_parts.append(\n",
    "                    f'Validation Cost: {validation_cost:.4f}')\n",
    "                message_parts.append(\n",
    "                    f'Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "            print(' | '.join(message_parts))\n",
    "    return result_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr2fr-aXtvYW"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics: dict, interval=100):\n",
    "    cost = metrics['costs']\n",
    "    accuracy = metrics['accuracies']\n",
    "    validation_cost = metrics['validation_costs']\n",
    "    validation_accuracies = metrics['validation_accuracies']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "    axes[0].plot(cost[::interval], label='Training Cost')\n",
    "    axes[0].plot(validation_cost[::interval], label='Validation Cost')\n",
    "    axes[0].set_ylabel('epoch')\n",
    "    axes[0].set_ylabel('Cost')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(accuracy[::interval], label='Training Accuracy')\n",
    "    axes[1].plot(validation_accuracies[::interval],\n",
    "                 label='Validation Accuracy')\n",
    "    axes[1].set_ylabel('epoch')\n",
    "    axes[1].set_ylabel(f'Accuracy')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # fig.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "    fig.set_size_inches(12, 9)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oJ69ziwDRjV"
   },
   "source": [
    "## Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpgmiiX5DRjW"
   },
   "outputs": [],
   "source": [
    "def get_cross_entropy_cost_function(binary=False):  \n",
    "    def binary_cross_entropy(Y, Y_hat, epsilon=1e-4):\n",
    "        assert Y.shape == Y_hat.shape, f'{Y.shape} != {Y_hat.shape}'\n",
    "        m = Y.shape[1]  # number of examples\n",
    "        cost = (-1/m) * (np.dot(Y, np.log(Y_hat+epsilon).T) +\n",
    "                        np.dot((1-Y), np.log(1-Y_hat+epsilon).T))\n",
    "        return cost.item()\n",
    "\n",
    "\n",
    "    def binary_cross_entropy_derivative(Y, Y_hat, epsilon=1e-4):\n",
    "        return ((1-Y)/(1-Y_hat+epsilon)) - (Y/(Y_hat+epsilon))\n",
    "\n",
    "    def softmax_crossentropy(Y, Y_hat, epsilon=1e-4):\n",
    "        assert Y.shape == Y_hat.shape, f'{Y.shape} != {Y_hat.shape}'\n",
    "        Y_hat = softmax(Y_hat)\n",
    "        m = Y.shape[1]\n",
    "        cost = 1/m * np.sum(-np.sum(Y*np.log(Y_hat+epsilon), axis=0))\n",
    "        return cost\n",
    "\n",
    "    def softmax_crossentropy_derivative(Y, Y_hat, epsilon=1e-7):\n",
    "        Y_hat = softmax(Y_hat)\n",
    "        return Y_hat-Y\n",
    "\n",
    "    if binary:\n",
    "        return binary_cross_entropy, binary_cross_entropy_derivative\n",
    "    else:\n",
    "        return softmax_crossentropy, softmax_crossentropy_derivative\n",
    "    \n",
    "def get_regularization_cost(layers: list, m: int, lambda_: float) -> float:\n",
    "    cost = 0\n",
    "    for layer in layers:\n",
    "        cost += np.sum(np.square(layer.W))\n",
    "    cost *= (1/m) * (lambda_/2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAIMipGtDRje"
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrlo6Y05DRjg"
   },
   "source": [
    "### Optimizer Superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPUFSsZ5DRjh"
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def initiate_parameters(self, layers: list):\n",
    "        pass\n",
    "    def update_parameters(self, layers: list):\n",
    "        pass\n",
    "    def update_optimizer(self, metrics: dict):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv5XsIRnDRjm"
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IK7NcD0EDRjn"
   },
   "outputs": [],
   "source": [
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update_parameters(self, layers: list):\n",
    "        for layer in layers:\n",
    "            layer.W = layer.W - self.learning_rate*layer.dW\n",
    "            layer.b = layer.b - self.learning_rate*layer.db\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGAENBXUDRjv"
   },
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ogaM6P0DRjw"
   },
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self, learning_rate, beta):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "\n",
    "    def initiate_parameters(self, layers: list):\n",
    "        for layer in layers:\n",
    "            layer.V_dW = np.zeros(layer.W.shape)\n",
    "            layer.V_db = np.zeros(layer.b.shape)\n",
    "        return True\n",
    "            \n",
    "    def update_parameters(self, layers: list):\n",
    "        for layer in layers:            \n",
    "            layer.V_dW = self.beta*layer.V_dW + (1-self.beta)*layer.dW\n",
    "            layer.V_db = self.beta*layer.V_db + (1-self.beta)*layer.db\n",
    "            \n",
    "            layer.W = layer.W - self.learning_rate*layer.V_dW\n",
    "            layer.b = layer.b - self.learning_rate*layer.V_db\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3fVRb6RDRj1"
   },
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O49wN-K5DRj2"
   },
   "outputs": [],
   "source": [
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, learning_rate, beta=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def initiate_parameters(self, layers: list):\n",
    "        for layer in layers:\n",
    "            layer.S_dW = np.zeros(layer.W.shape)\n",
    "            layer.S_db = np.zeros(layer.b.shape)\n",
    "        return True\n",
    "            \n",
    "    def update_parameters(self, layers: list):\n",
    "        for layer in layers:            \n",
    "            layer.S_dW = self.beta*layer.S_dW + (1-self.beta)*np.square(layer.dW)\n",
    "            layer.S_db = self.beta*layer.S_db + (1-self.beta)*np.square(layer.db)\n",
    "            \n",
    "            layer.W = layer.W - self.learning_rate*(layer.dW/(np.sqrt(layer.S_dW)+self.epsilon))\n",
    "            layer.b = layer.b - self.learning_rate*(layer.db/(np.sqrt(layer.S_db)+self.epsilon))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6othBMpDRj8"
   },
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPrPM09fDRj9"
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = 1e-7\n",
    "        self.counter = 0\n",
    "\n",
    "    def initiate_parameters(self, layers: list):\n",
    "        self.counter = 0\n",
    "        for layer in layers:\n",
    "            layer.V_dW = np.zeros(layer.W.shape)\n",
    "            layer.V_db = np.zeros(layer.b.shape)\n",
    "            layer.S_dW = np.zeros(layer.W.shape)\n",
    "            layer.S_db = np.zeros(layer.b.shape)\n",
    "            \n",
    "            layer.V_corrected_dW = np.zeros(layer.W.shape)\n",
    "            layer.V_corrected_db = np.zeros(layer.b.shape)\n",
    "            layer.S_corrected_dW = np.zeros(layer.W.shape)\n",
    "            layer.S_corrected_db = np.zeros(layer.b.shape)\n",
    "        return True\n",
    "            \n",
    "    def update_parameters(self, layers: list):\n",
    "        for layer in layers:            \n",
    "            layer.V_dW = self.beta_1*layer.V_dW + (1-self.beta_1)*layer.dW\n",
    "            layer.V_db = self.beta_1*layer.V_db + (1-self.beta_1)*layer.db\n",
    "            layer.S_dW = self.beta_2*layer.S_dW + (1-self.beta_2)*np.square(layer.dW)\n",
    "            layer.S_db = self.beta_2*layer.S_db + (1-self.beta_2)*np.square(layer.db)\n",
    "            \n",
    "            # Apply bias correction\n",
    "            momentum_correction = 1/(1-self.beta_1**(self.counter+1))\n",
    "            rmsprop_correction = 1/(1-self.beta_2**(self.counter+1))\n",
    "\n",
    "            layer.V_corrected_dW = layer.V_dW*momentum_correction\n",
    "            layer.V_corrected_db = layer.V_db*momentum_correction\n",
    "            layer.S_corrected_dW = layer.S_dW*rmsprop_correction\n",
    "            layer.S_corrected_db = layer.S_db*rmsprop_correction\n",
    "        \n",
    "            layer.W = layer.W - self.learning_rate*(layer.V_corrected_dW/(np.sqrt(layer.S_corrected_dW)+self.epsilon))\n",
    "            layer.b = layer.b - self.learning_rate*(layer.V_corrected_db/(np.sqrt(layer.S_corrected_db)+self.epsilon))\n",
    "        self.counter += 1\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kArP_ESqtvYG"
   },
   "source": [
    "## Main Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVvWPqcotvYO"
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def __init__(self, model_structure: ModelStructure, binary=False):\n",
    "        self.input_features = model_structure.layers[0].units_number\n",
    "        self.layers = initialize(model_structure.layers)\n",
    "        self.layers_backup = copy.deepcopy(self.layers)\n",
    "        self.cost_function = get_cross_entropy_cost_function(binary=binary)[0]\n",
    "        self.cost_function_derivative = get_cross_entropy_cost_function(binary=binary)[1]\n",
    "        self.accuracy_func = get_accuracy_function(binary=binary)\n",
    "        self.predict_function = get_predict_function(apply_softmax=not binary)\n",
    "        self.metrics = {\n",
    "                        'costs': [],\n",
    "                        'accuracies': [],\n",
    "                        'validation_costs': [],\n",
    "                        'validation_accuracies': [],\n",
    "                        'total_trained_epochs': 0,\n",
    "                        'current_trained_epochs': 0,\n",
    "                        }\n",
    "\n",
    "    def undo_update(self):\n",
    "        self.layers = copy.deepcopy(self.layers_backup)\n",
    "        self.metrics['costs'].pop()\n",
    "        self.metrics['accuracies'].pop()\n",
    "        self.metrics['validation_costs'].pop()\n",
    "        self.metrics['validation_accuracies'].pop()\n",
    "        self.metrics['total_trained_epochs'] -= 1\n",
    "        self.metrics['current_trained_epochs'] -= 1\n",
    "        print('Last Successful Parameters Recovered')\n",
    "\n",
    "    def feed_forward(self, X: np.ndarray):\n",
    "        assert X.shape[0] == self.input_features\n",
    "        A_prev = X\n",
    "        for layer in self.layers:\n",
    "            Z = np.dot(layer.W, A_prev) + layer.b\n",
    "            A = layer.activation_function(Z)\n",
    "            A_prev = A\n",
    "        Y_hat = A_prev\n",
    "        return Y_hat\n",
    "\n",
    "    def feed_forward_train(self, X: np.ndarray):\n",
    "        assert X.shape[0] == self.input_features\n",
    "        A_prev = X\n",
    "        for layer in self.layers:\n",
    "            layer.A_previous = A_prev\n",
    "            layer.Z = np.dot(layer.W, A_prev) + layer.b\n",
    "            A_raw = layer.activation_function(\n",
    "                layer.Z)  # Before applying dropout\n",
    "            layer.D = np.random.rand(\n",
    "                *A_raw.shape) < layer.keep_prob  # Dropout Mask\n",
    "            layer.A = (A_raw * layer.D) / layer.keep_prob\n",
    "            A_prev = layer.A\n",
    "        Y_hat = A_prev\n",
    "        return Y_hat\n",
    "\n",
    "    def back_propagate(self, cost_derivative, lambda_):\n",
    "        dA_prev = cost_derivative\n",
    "        m = self.layers[0].Z.shape[1]  # number of examples\n",
    "        for layer in self.layers[::-1]:\n",
    "            layer.dA = (dA_prev*layer.D) / layer.keep_prob\n",
    "            layer.dZ = layer.dA * layer.activation_function_derivative(layer.Z)\n",
    "            layer.dW = (1/m) * np.dot(layer.dZ,\n",
    "                                      layer.A_previous.T) + (lambda_/m)*layer.W\n",
    "            layer.db = 1/m * np.sum(layer.dZ, axis=1, keepdims=True)\n",
    "            dA_prev = np.dot(layer.W.T, layer.dZ)\n",
    "        return True\n",
    "\n",
    "    def fit_minibatch(self, mini_X, mini_Y, lambda_, optimizer):\n",
    "        mini_Y_hat = self.feed_forward_train(mini_X)\n",
    "        cost_derivative = self.cost_function_derivative(mini_Y, mini_Y_hat)\n",
    "        self.back_propagate(cost_derivative, lambda_)\n",
    "        optimizer.update_parameters(self.layers)\n",
    "        return True\n",
    "\n",
    "    def fit(self, X, Y, epochs, batch_size, optimizer,\n",
    "            lambda_=0,\n",
    "            validation_data=None,\n",
    "            metrics_printer=None):\n",
    "        \n",
    "        # Assert Shapes\n",
    "        assert X.shape[1] == Y.shape[1]        \n",
    "        m = X.shape[1]\n",
    "        if validation_data:\n",
    "            assert type(validation_data) == tuple\n",
    "            assert type(validation_data[0]) == np.ndarray\n",
    "            assert type(validation_data[1]) == np.ndarray\n",
    "            assert validation_data[0].shape[0] == X.shape[0]\n",
    "            assert validation_data[1].shape[0] == Y.shape[0]\n",
    "            assert validation_data[0].shape[1] == validation_data[1].shape[1]\n",
    "\n",
    "        self.metrics['current_trained_epochs'] = 0\n",
    "        optimizer.initiate_parameters(self.layers)\n",
    "        for _ in range(1, epochs+1):\n",
    "            self.layers_backup = copy.deepcopy(self.layers)           \n",
    "            mini_batchs = mini_batch_generator(X, Y, batch_size)\n",
    "            for mini_X, mini_Y in mini_batchs:\n",
    "                self.fit_minibatch(mini_X, mini_Y, lambda_, optimizer)\n",
    "\n",
    "            regularization_cost = get_regularization_cost(self.layers, m,\n",
    "                                                          lambda_)\n",
    "\n",
    "            train_Y_hat = self.feed_forward(X)\n",
    "            train_cost = self.cost_function(Y, train_Y_hat)\n",
    "            train_cost += regularization_cost\n",
    "            train_prediction = self.predict_function(train_Y_hat)\n",
    "            train_accuracy = self.accuracy_func(train_prediction, Y)\n",
    "            self.metrics['costs'].append(train_cost)\n",
    "            self.metrics['accuracies'].append(train_accuracy)\n",
    "\n",
    "            if validation_data:\n",
    "                validation_X = validation_data[0]\n",
    "                validation_Y = validation_data[1]\n",
    "                validation_Y_hat = self.feed_forward(validation_X)\n",
    "                validation_cost = self.cost_function(validation_Y,\n",
    "                                                     validation_Y_hat) \n",
    "                validation_cost += regularization_cost\n",
    "                validation_prediction = self.predict_function(validation_Y_hat)\n",
    "                validation_accuracy = self.accuracy_func(validation_prediction,\n",
    "                                                         validation_Y)\n",
    "                self.metrics['validation_costs'].append(validation_cost)\n",
    "                self.metrics['validation_accuracies'].append(validation_accuracy)\n",
    "\n",
    "            self.metrics['total_trained_epochs'] += 1\n",
    "            self.metrics['current_trained_epochs'] += 1\n",
    "\n",
    "            if metrics_printer:\n",
    "                metrics_printer(self.metrics)\n",
    "                         \n",
    "            optimizer.update_optimizer(self.metrics)            \n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNMSqla9L9PF"
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BRPI1CUL9PG"
   },
   "outputs": [],
   "source": [
    "def get_dataset(dataset_path):\n",
    "    data = np.load(dataset_path)\n",
    "    X_train = data['X_train']\n",
    "    Y_train = data['Y_train']\n",
    "    X_test = data['X_test']\n",
    "    Y_test = data['Y_test']\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IbnZyMtL9PM"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(Y, C):\n",
    "    assert Y.ndim == 1\n",
    "    m = Y.shape[0]\n",
    "    shape = (C, m)\n",
    "    result = np.zeros(shape)\n",
    "    result[Y,np.arange(m)] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucWtk2EqL9PT"
   },
   "outputs": [],
   "source": [
    "mnist_path = Path.joinpath(Path('datasets'), Path('mnist.npz'))\n",
    "X_train, Y_train, X_test, Y_test = get_dataset(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Kmvxa0ZL9Pb"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "Y_train = one_hot_encode(Y_train, 10)\n",
    "Y_test = one_hot_encode(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "slwXNv9bL9Pu",
    "outputId": "5baaa8c1-7cf0-433d-efe3-a0e88b1b59d9"
   },
   "outputs": [],
   "source": [
    "index = 85\n",
    "print(Y_train[:,index])\n",
    "PIL.Image.fromarray(X_train[:, index].reshape(28, 28)).resize((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbM5Vhi8L9P5"
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdJ3ca4hL9P-",
    "outputId": "8f27581a-8546-428b-844c-85afebf72d3b"
   },
   "outputs": [],
   "source": [
    "structure = ModelStructure()\n",
    "structure.add_input_layer(X_train.shape[0])\n",
    "structure.add_layer(128, 'relu', keep_prob=0.8)\n",
    "structure.add_layer(10)\n",
    "structure.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ma_VV8lL9QF"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "model = Classifier(structure)\n",
    "\n",
    "gd_optimizer = GradientDescent(learning_rate=0.01)\n",
    "\n",
    "momentum_optimizer = Momentum(learning_rate=0.001, beta=0.9)\n",
    "rmsprop_optimizer = RMSprop(learning_rate=0.0002, beta=0.999)\n",
    "adam_optimizer = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "W6qtaqfO7CKe",
    "outputId": "66b5b40e-60f0-4511-b1db-940b23fff015"
   },
   "outputs": [],
   "source": [
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "epochs=5\n",
    "metrics = model.fit(X_train, Y_train, epochs, optimizer=adam_optimizer,\n",
    "                    batch_size=32, validation_data=(X_test, Y_test),\n",
    "                    lambda_=0, metrics_printer=print_metrics(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jABmmKLL9Qr",
    "outputId": "e2bc8ae0-47c4-4b2b-9055-c8a19bb5ac33"
   },
   "outputs": [],
   "source": [
    "categorical_accuracy(predict(model_test.feed_forward(X_test)), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-yiEWwkL9Qw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deepLearningNotebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
